---
title: "HW4 - High income household prediction"
output: html_document
Author: Justin Sarna
Class: MIS 680
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
acs <- read.table("http://jaredlander.com/data/acs_ny.csv",sep=",",
                  header=TRUE, stringsAsFactors=TRUE)
```

```{r}
library(ggplot2)
library(useful)
library(caret)
library(ISLR)
library(scales)
library(plyr)
library(rpart)
library(rpart.plot)
library(class)
```

Let's assume our goal is to build a model to predict if household income is greater 
than $250,000 per year.


## Task 1: Data preparation

We start by building a binary response variable.

```{r}
acs$HighIncome <- as.numeric(with(acs, FamilyIncome >= 250000))
head(acs)
tail(acs)
```

## Before splitting data, I am going to add/modify some variables for use in my models

1) Food Stamp to binary integer for linear regression
2) Own home to binary integer for linear regression
3) Family type to numerical for linear regression later
```{r}
# (yes = 1, no = 0)
acs$foodstamp_binary <- ifelse(acs$FoodStamp == "Yes",1,0)
  # Option 2 for doing this - I did not use this. Does not add new column but modifes exisiting column
  # levels(acs$FoodStamp) <- c("0","1")

# own = 1, rent = 0
acs$own_home <- ifelse(acs$OwnRent == "Rented",0, ifelse(acs$FamilyIncome == "Mortgage",1,2))
# married = 1, male head = 2, female head = 3
acs$family_type_cat <- ifelse(acs$FamilyType == "Married",1, ifelse(acs$FamilyIncome == "Female Head",2,3))
```

### Based on groupby and plots (completed later) create new variables for potential use

```{r}
acs$InsuranceHigh <- (acs$Insurance > 1000) * 1
acs$NumWorkers2 <- (acs$NumWorkers == 2) * 1
acs$HouseCostsHigh <- (acs$HouseCosts > 1000) * 1
acs$high_electric <- (acs$ElectricBill > 350) * 1
```

### Break it into a training and test set with an 80/20 split.

```{r}
set.seed(447)
testrecs <- sample(nrow(acs),0.2 * nrow(acs))
acs_test <- acs[testrecs,]
acs_fit <- acs[-testrecs,]  
```

Create binary variable where 1 = not on food stamps & not renting & married

```{r}
acs$HI_pred1 <- 0
acs$HI_pred1[acs_test$FoodStamp == 'No' & acs_test$OwnRent != 'Rented' & acs_test$FamilyType == 'Married'] <- 1
```

```{r}
# I like this visualization for a quick visual of what I'm dealing with before digging in (modified from class notes)
ggplot(acs,aes(x=FamilyIncome)) + geom_density(fill="#31a354", color="#31a354") +
  geom_vline(xintercept=250000) + scale_x_continuous(label=multiple.dollar, limits=c(0,1000000))
```

### Interesting test I found for testing data normality - downside is it has a 5000 observation limit
If p-value is less than .05 (or chosen significance level) then sample is NOT normally distributed
This is not relevant here, but worth keeping for future
The plot shows that there is a clear left skewned distribution - expected but still nice to include

```{r}
shapiro.test(acs_test$FamilyIncome)
```

# Task 2: Preliminary EDA and feature engineering

Before trying to build any classification models, you should do some exploratory data analysis to
try to get a sense of which variables might be useful for trying to predict cases where FamilyIncome >= 250000. You should use a combination
of group by analysis (i.e. plyr or dplyr or similar) and plotting.  

If you decide you'd like to create some new variables (feature engineering), feel free to do that. Just document what you did and why you did it. 

```{r}
# create matrix of scatterplots
# pairs(acs_test[,1:19])
```

```{r}
# Get some summary stats on each variable
summary(acs_fit)
```
```{r}
ggplot(acs_fit) + geom_histogram(aes(x=own_home), fill = "gray")
```
# Scatterplots
```{r}
# ADD NOTES HERE
ggplot(data=acs_fit) + geom_point(aes(x=NumWorkers, y=FamilyIncome))

# scatter plot shows that those not on foodstamps tend to have higher income = duh, but relevant for model later
ggplot(data=acs_fit) + geom_point(aes(x=foodstamp_binary, y=FamilyIncome))
ggplot(data=acs_fit) + geom_point(aes(x=NumWorkers, y=FamilyIncome))

# notice that there are very few observations with male head type. Female head has lower income 
ggplot(data=acs_fit) + geom_point(aes(x=family_type_cat, y=FamilyIncome))

# ADD NOTES HERE
ggplot(data=acs_fit) + geom_point(aes(x=HouseCosts, y=FamilyIncome))
```
# Box Plots
```{r}
# ADD NOTES
ggplot(data=acs_fit) + geom_boxplot(aes(x=NumWorkers, y=FamilyIncome))
```

```{r}
# ADD NOTES
ggplot(acs_fit) + geom_density(aes(x=acs_fit$FamilyIncome)) + scale_x_continuous(labels=dollar)
```

```{r}
# ADD NOTES
ggplot(acs_fit) + geom_density(aes(x=acs_fit$HouseCosts)) + scale_x_continuous(labels=dollar)
```
```{r}
# ADD NOTES
ggplot(acs_fit) + geom_density(aes(x=acs_fit$NumChildren)) + scale_x_continuous()
```
```{r}
# ADD NOTES
ggplot(acs_fit) + geom_density(aes(x=acs_fit$FamilyIncome)) + scale_x_log10(breaks =c(100,1000,10000,100000), labels=dollar) + annotation_logticks(sides="bt")
```
```{r}
# ADD NOTES
ggplot(acs_fit) + geom_density(aes(x=acs_fit$HouseCosts)) + scale_x_log10(breaks =c(100,1000,10000,100000), labels=dollar) + annotation_logticks(sides="bt")
```
```{r}
# ADD NOTES
ggplot(acs_fit, aes(x=acs_fit$Insurance, y=acs_fit$FamilyIncome)) +geom_point() + geom_smooth()
```
```{r}
# ADD NOTES
ggplot(acs_fit) + geom_density(aes(x=acs_fit$ElectricBill)) + scale_x_log10(breaks =c(100,1000,10000,100000), labels=dollar) + annotation_logticks(sides="bt")
ggplot(acs_fit, aes(x=acs_fit$ElectricBill, y=acs_fit$FamilyIncome)) +geom_point() + geom_smooth()
```

# GROUP BY ANALYSIS

```{r}
# This shows a good spread or range of each family type group. This will lend itself well to being included in my analysis
ddply(acs_fit,.(FamilyType),summarise,family_type_count=length(FamilyIncome))
```
```{r}
# Interesting look at mean income of family type grouped with home ownership type
ddply(acs_fit,.(FamilyType,OwnRent), summarise, mean_income=mean(FamilyIncome))
```
```{r}
ddply(acs_fit,.(FamilyType,FoodStamp), summarise, mean_income=mean(FamilyIncome))
```
```{r}
# simple look at mean income by foodstamp. Obvious results, but at the same time surprising to find that mean income 
# for those on food stamps in near $50k
ddply(acs_fit,.(FoodStamp), summarise, mean_income=mean(FamilyIncome))
```
```{r}
ddply(acs_fit,.(FoodStamp,NumBedrooms), summarise, mean_income=mean(FamilyIncome), num_bedrooms=mean(NumBedrooms))
```
```{r}
#ddply(acs,.(NumBedrooms,NumChildren,NumPeople,NumRooms,NumUnits,NumVehicles,NumWorkers), summarise, mean_income=mean(FamilyIncome))
```
```{r}
ddply(acs_fit,.(OwnRent), summarise, mean_income=mean(FamilyIncome))
```

# Count (family income) by various important indicators/variables

```{r}
# Family Type
tapply(acs_fit$FamilyIncome,acs_fit$FamilyType,length)
tapply(acs_fit$FamilyIncome,acs_fit$FamilyType,mean)
```

```{r}
# Own/Rent
tapply(acs_fit$FamilyIncome,acs_fit$OwnRent,length)
tapply(acs_fit$FamilyIncome,acs_fit$OwnRent,mean)
```

```{r}
# Insurance
tapply(acs_fit$FamilyIncome,acs_fit$FoodStamp,length)
tapply(acs_fit$FamilyIncome,acs_fit$FoodStamp,mean)
```

# Task 3 - Building predictive classifier models using the entire training dataset

Let's start by building a *null* model in which you simply predict that everyone's
income is < 250000 (since the majority of incomes are less than 250000).

```{r}
acs$null_model <- 0
```


Create a confusion matrix table and compute the overall accuracy of this model
as well as its sensitivity and specificity.

```{r}
library(caret)
table(acs$HighIncome, acs$null_model)
prop.table(table(acs$HighIncome, acs$null_model))
```
```{r}
confusionMatrix(as.factor(acs$null_model), as.factor(acs$HighIncome), positive = "1")
```

We would like to build a more accurate model than this.
Your job is to build classifiers to predict the binary HighIncome we created. 
You will be using three different classification
techniques:
* decision trees (use `rpart` package - see Kaggle Titanic example from StatModels2 or session on trees)
* logistic regression (see logistic regression examples we did in StatModels2)
* k-nearest neighbor or some other technique (see kNN example we did in StatModels2)
For each technique, you should:
* build a few models with the training data
* create confusion matrices (using `caret` package) to pick the best fit model for each technique
* use your three best fit models (one from each technique) to predict using the test dataset and evaluate which of the models performs the best
* write a few paragraphs discussing what you did and what you found. In particular, how difficult is it to predict HighIncome? Did one of the techniques outperform the other two?

# LOGISTIC REGRESSION

1) specify the model
2) show summary results
3) check fitted values from the model
4) set variable equal to fitted values with probability greater than 50%
5) show fitted values for visual check of model
6) confusion matrix
```{r}
# logistic regression model 1
logmod1 <- glm(HighIncome ~ FamilyType + NumVehicles + OwnRent + Insurance + YearBuilt, data=acs_fit, family=binomial(link="logit"))
summary(logmod1)
logmod1$fitted.values[1:10]
yhat_logmod1 <- (logmod1$fitted.values > 0.05) * 1
#logmod1$y[1:200]
summary(as.factor(yhat_logmod1))
summary(logmod1$fitted.values)
confusionMatrix(as.factor(yhat_logmod1), as.factor(acs_fit$HighIncome), positive = "1")
```

```{r}
# logistic regression model 2
logmod2 <- glm(HighIncome ~ FamilyType + FoodStamp + OwnRent, data=acs_fit, family=binomial(link="logit"))
summary(logmod2)
logmod2$fitted.values[1:10]
yhat_logmod2 <- (logmod2$fitted.values > 0.5) * 1
#logmod2$y[1:50]
confusionMatrix(as.factor(yhat_logmod2), as.factor(acs_fit$HighIncome), positive = "1")
```

```{r}
# logistic regression model 3
logmod3 <- glm(HighIncome ~ InsuranceHigh + NumWorkers2 + HouseCostsHigh + FoodStamp + OwnRent, data=acs_fit, family=binomial(link="logit"))
summary(logmod3)
#logmod3$fit[18100:18196]
yhat_logmod3 <- (logmod3$fit > 0.5) * 1
#logmod3$y[18000:18196]
confusionMatrix(as.factor(yhat_logmod3), as.factor(acs_fit$HighIncome), positive = "1")
```

```{r}
# logistic regression model 4
logmod4 <- glm(HighIncome ~ InsuranceHigh + NumWorkers2 + HouseCostsHigh, data=acs_fit, family=binomial(link="logit"))
summary(logmod4)
#logmod4$fit[18100:18196]
yhat_logmod4 <- (logmod4$fit > 0.5) * 1
#logmod4$y[18000:18196]
confusionMatrix(as.factor(yhat_logmod4), as.factor(acs_fit$HighIncome), positive = "1")
```
```{r}
# logistic regression model 5
logmod5 <- glm(HighIncome ~ HouseCostsHigh, data=acs_fit, family=binomial(link="logit"))
summary(logmod5)
logmod5$fit[18100:18196]
yhat_logmod5 <- (logmod4$fit > 0.5) * 1
logmod5$y[18000:18196]
confusionMatrix(as.factor(yhat_logmod5), as.factor(acs_fit$HighIncome), positive = "1")
```

# Linear Regression with predictions

```{r}
# Created a linear regression for fun to estimate family income with test data set
linear_mod1 <- lm(FamilyIncome ~ FamilyType + FoodStamp + OwnRent + HouseCosts + Insurance + ElectricBill + NumRooms, data=acs_fit)
summary(linear_mod1)
```

```{r}
# Created a linear regression for fun to estimate family income with test data set
linear_mod2 <- lm(FamilyIncome ~ Insurance + HouseCosts + ElectricBill + NumWorkers + FamilyType + FoodStamp + OwnRent + NumBedrooms + NumChildren + NumRooms + NumPeople + NumVehicles + Language, data=acs_fit)
summary(linear_mod2)
linear_mod2$fitted.values[1:25]
acs_fit$linest_HighIncome <- ifelse(linear_mod2$fit > 250000,1,0)
confusionMatrix(as.factor(acs_fit$linest_HighIncome), as.factor(acs_fit$HighIncome), positive = "1")

# My linear model does estimate negative incomes, which is not logically or stastically sound
# However, the purpose is to find best predictive model. So I ignored this to see how accurate I could predict High Income
```

```{r}
# Created a linear regression for fun to estimate family income with test data set
linear_mod3 <- lm(FamilyIncome ~ Insurance + HouseCosts + ElectricBill + NumWorkers + FamilyType + FoodStamp + OwnRent + NumBedrooms + NumChildren + NumRooms + NumPeople + NumVehicles + Language, data=acs)
summary(linear_mod3)
linear_mod2$fitted.values[1:25]
acs$linest_HighIncome <- ifelse(linear_mod3$fit > 250000,1,0)
confusionMatrix(as.factor(acs$linest_HighIncome), as.factor(acs$HighIncome), positive = "1")

# My linear model does estimate negative incomes, which is not logically or stastically sound
# However, the purpose is to find best predictive model, and income is merely used to predict if HighIncome.
# So I ignored illogical family income to see how accurate I could predict High Income
```

# DECISION TREES

```{r}
# Decision tree 1
tree1 <- rpart(HighIncome ~ FamilyType + HouseCosts + NumWorkers2 + OwnRent + Insurance + NumWorkers2 + YearBuilt + NumBedrooms, data=acs_fit, method="class")
rpart.plot(tree1)
tree1
head(predict(tree1))
head(predict(tree1, type="class"))
tree1_cm <- confusionMatrix(predict(tree1, type="class"), acs_fit$HighIncome, positive = "1")
tree1_cm
```
```{r}
# Decision tree 2
tree2 <- rpart(HighIncome ~ FoodStamp + Insurance + FamilyType, data=acs_fit, method="class", control=rpart.control(minsplit=2, cp=0))
rpart.plot(tree2)
tree2
head(predict(tree2))
head(predict(tree2, type="class"))
tree2_cm <- confusionMatrix(predict(tree2, type="class"), acs_fit$HighIncome, positive = "1")
tree2_cm
```
```{r}
# Decision tree 3
tree3 <- rpart(HighIncome ~ Insurance + ElectricBill + HouseCosts, data=acs_fit, method="class", control=rpart.control(minsplit=2, cp=.005))
rpart.plot(tree3)
tree3
head(predict(tree3))
head(predict(tree3, type="class"))
tree3_cm <- confusionMatrix(predict(tree3, type="class"), acs_fit$HighIncome, positive = "1")
tree3_cm
```

```{r}
# Decision tree 4
tree4 <- rpart(HighIncome ~ Insurance + ElectricBill + HouseCosts + NumBedrooms + NumChildren + NumPeople + NumRooms + NumVehicles + NumWorkers + FoodStamp + OwnRent + ElectricBill + HouseCosts, data=acs_fit, method="class", control=rpart.control(minsplit=2, cp=0))
rpart.plot(tree4)
tree4
head(predict(tree4))
head(predict(tree4, type="class"))
tree4_cm <- confusionMatrix(predict(tree4, type="class"), acs_fit$HighIncome, positive = "1")
tree4_cm
# This crazy tree is more accurate(only missed one prediction). However, it is highly sensetive. 
# I would not choose this tree due to such .99 sensetivity
```

### Tree Comparison

```{r}
# make predictions using test data
tree1_pred <- predict(tree1, acs_test, type="class" )
tree2_pred <- predict(tree2, acs_test, type="class" ) 
tree3_pred <- predict(tree3, acs_test, type="class" ) 
tree4_pred <- predict(tree4, acs_test, type="class" )

# Confusion matrices
tree_cm1_pred <- confusionMatrix(tree1_pred, acs_test$HighIncome, positive = "1")
tree_cm2_pred <- confusionMatrix(tree2_pred, acs_test$HighIncome, positive = "1")
tree_cm3_pred <- confusionMatrix(tree3_pred, acs_test$HighIncome, positive = "1")
tree_cm4_pred <- confusionMatrix(tree4_pred, acs_test$HighIncome, positive = "1")

# Display comparison of accuracy of each decision tree 
sprintf("Tree1: Fit acc = %.3f Pred acc = %.3f",tree1_cm$overall['Accuracy'], tree_cm1_pred$overall['Accuracy'])
sprintf("Tree2: Fit acc = %.3f Pred acc = %.3f",tree2_cm$overall['Accuracy'], tree_cm2_pred$overall['Accuracy'])
sprintf("Tree3: Fit acc = %.3f Pred acc = %.3f",tree3_cm$overall['Accuracy'], tree_cm3_pred$overall['Accuracy'])
sprintf("Tree4: Fit acc = %.3f Pred acc = %.3f",tree4_cm$overall['Accuracy'], tree_cm4_pred$overall['Accuracy'])
```


# k-nearest neighbor

## First Normalize the dataset
```{r}
normalize <- function(x) {
num <- x - min(x)
denom <- max(x) - min(x)
return (num/denom)
}
```
```{r}
acs_numericals <- data.frame(acs$HighIncome, acs$FamilyIncome, acs$NumBedrooms, acs$NumChildren, acs$NumPeople, acs$NumRooms, acs$NumVehicles, acs$NumWorkers, acs$HouseCosts, acs$ElectricBill, acs$Insurance)
acs_norm <- as.data.frame(lapply(acs_numericals[2:10], normalize))
acs_norm$HighIncome1 <- c(acs$HighIncome)
```
```{r}
# Count rows
m <- nrow(acs_numericals)

# Create index of random row nums for the validation set
val <- sample(1:m, size = round(m/3))

# Create the learning and validation sets
acsNorm_learn <- acs_norm[-val,]
acsNorm_valid <- acs_norm[val,]
```

```{r}
summary(acs_norm)
```

```{r}
acs_knn <- knn(acsNorm_learn[,2:10], acsNorm_valid[,2:10], acsNorm_learn$HighIncome, k=5, prob = TRUE)
acs_knn

# Show the classification matrix
table(acsNorm_valid$HighIncome, acs_knn)
```

```{r}
pcol <- as.character(as.numeric(acsNorm_valid$HighIncome1))
pairs(acsNorm_valid[1:10], pch = pcol, col = c("green3", "red")
  [(acsNorm_valid$HighIncome1 != acs_knn)+1])
```


## HACKER EXTRAS

There's tons of opportunities for extra credit here. A couple of ideas are:

* try other classification techniques (see the ISLR textbook ([http://www-bcf.usc.edu/~gareth/ISL/](http://www-bcf.usc.edu/~gareth/ISL/))) such as neural nets, random forests or discriminant analysis
* In Task 3, use k-fold cross-validation for the logistic regression models

You'll be turning in your Rmd file with all of your work.