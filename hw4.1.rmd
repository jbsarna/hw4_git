---
title: "HW4 - High income household prediction"
output: html_document
Author: Justin Sarna
Class: MIS 680
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
HAS THIS LINE CHANGED AGAIN or now?
You'll be doing all of your work in this R Markdown document. Please rename the file
to include name(s).

## Background

This assignment is based on one of the datasets we looked at in the session on logistic regression - data from the American Community Survey. It's also used
in Chapter 17 of R for Everyone.

```{r}
acs <- read.table("http://jaredlander.com/data/acs_ny.csv",sep=",",
                  header=TRUE, stringsAsFactors=TRUE)
```

```{r}
library(ggplot2)
library(useful)
library(caret)
library(ISLR)
library(scales)
library(plyr)
library(rpart)
library(rpart.plot)
```

Let's assume our goal is to build a model to predict if household income is greater 
than $250,000 per year.


## Task 1: Data preparation

We start by building a binary response variable.

```{r}
acs$HighIncome <- as.numeric(with(acs, FamilyIncome >= 250000))
head(acs)
tail(acs)
```

```{r}
## Before splitting data, I am going to add/modify some variables for use in my models
# First I want "food stamps" to be a binary variable not string (yes = 1, no = 0)
# Option 1 for doing this is below. Used this for easy visual analysis check if correct. This adds a new column
acs$foodstamp_binary <- ifelse(acs$FoodStamp == "Yes",1,0)

# Option 2 for doing this - I did not use this. Does not add new column but modifes exisiting column
#levels(acs$FoodStamp) <- c("0","1")

# Next I want to change factor own/rent to numeric binary (own = 1, rent = 0)
acs$own_home <- ifelse(acs$OwnRent == "Rented",0, ifelse(acs$FamilyIncome == "Mortgage",1,2))

# Next Family type is factor with three levels, and want as numerical data for possible use in regression or model testing.
# married = 1, male head = 2, female head = 3
acs$family_type_cat <- ifelse(acs$FamilyType == "Married",1, ifelse(acs$FamilyIncome == "Female Head",2,3))
```
# BASED ON PLOTS AND GROUPBY I AM GOING TO CREATE SOME NEW VARIABLE FOR POTENTIAL USE

```{r}
acs$InsuranceHigh <- (acs$Insurance > 1000) * 1
acs$NumWorkers2 <- (acs$NumWorkers == 2) * 1
acs$HouseCostsHigh <- (acs$HouseCosts > 1000) * 1

acs$HI_pred1 <- 0
acs$HI_pred1[acs_test$FoodStamp == 'No' & acs_test$OwnRent != 'Rented' & acs_test$FamilyType == 'Married'] <- 1

acs$high_electric <- (acs$ElectricBill > 350) * 1
```
Let's break it into a training and test set with an 80/20 split.

```{r}
set.seed(447)
testrecs <- sample(nrow(acs),0.2 * nrow(acs))
acs_test <- acs[testrecs,]
acs_fit <- acs[-testrecs,]  
```

```{r}
# I like this visualization for a quick visual of what I'm dealing with before digging in (modified from class notes)
ggplot(acs,aes(x=FamilyIncome)) + geom_density(fill="#31a354", color="#31a354") +
  geom_vline(xintercept=250000) + scale_x_continuous(label=multiple.dollar, limits=c(0,1000000))
```
```{r}
# Interesting test I found for testing data normality - downside is it has a 5000 observation limit
# If p-value is less than .05 (or chosen significance level) then sample is NOT normally distributed
# This is not relevant here, but worth keeping for future
# The plot above shows that there is a clear left skewned distribution
shapiro.test(acs_test$FamilyIncome)
```


## Task 2: Preliminary EDA and feature engineering

Before trying to build any classification models, you should do some exploratory data analysis to
try to get a sense of which variables might be useful for trying to predict cases where FamilyIncome >= 250000. You should use a combination
of group by analysis (i.e. plyr or dplyr or similar) and plotting.  

If you decide you'd like to create some new variables (feature engineering), feel free to do that. Just document what you did and why you did it. 

```{r}
# create matrix of scatterplots
# pairs(acs_test[,1:19])
```

```{r}
# Get some summary stats on each variable
summary(acs_fit)
```
```{r}
ggplot(acs_fit) + geom_histogram(aes(x=own_home), fill = "gray")
```

```{r}
## Scatterplots
ggplot(data=acs_fit) + geom_point(aes(x=NumWorkers, y=FamilyIncome))

# scatter plot shows that those not on foodstamps tend to have higher income = duh, but relevant for model later
ggplot(data=acs_fit) + geom_point(aes(x=foodstamp_binary, y=FamilyIncome))
ggplot(data=acs_fit) + geom_point(aes(x=NumWorkers, y=FamilyIncome))

# notice that there are very few observations with male head type. Female head has lower income 
ggplot(data=acs_fit) + geom_point(aes(x=family_type_cat, y=FamilyIncome))

ggplot(data=acs_fit) + geom_point(aes(x=HouseCosts, y=FamilyIncome))
```

```{r}
# box plots
ggplot(data=acs_fit) + geom_boxplot(aes(x=NumWorkers, y=FamilyIncome))

```

```{r}
ggplot(acs_fit) + geom_density(aes(x=acs_fit$FamilyIncome)) + scale_x_continuous(labels=dollar)
```

```{r}
ggplot(acs_fit) + geom_density(aes(x=acs_fit$HouseCosts)) + scale_x_continuous(labels=dollar)
```
```{r}
ggplot(acs_fit) + geom_density(aes(x=acs_fit$NumChildren)) + scale_x_continuous()
```
```{r}
ggplot(acs_fit) + geom_density(aes(x=acs_fit$FamilyIncome)) + scale_x_log10(breaks =c(100,1000,10000,100000), labels=dollar) + annotation_logticks(sides="bt")
```
```{r}
ggplot(acs_fit) + geom_density(aes(x=acs_fit$HouseCosts)) + scale_x_log10(breaks =c(100,1000,10000,100000), labels=dollar) + annotation_logticks(sides="bt")
```

```{r}
ggplot(acs_fit, aes(x=acs_fit$Insurance, y=acs_fit$FamilyIncome)) +geom_point() + geom_smooth()
```
```{r}
ggplot(acs_fit) + geom_density(aes(x=acs_fit$ElectricBill)) + scale_x_log10(breaks =c(100,1000,10000,100000), labels=dollar) + annotation_logticks(sides="bt")

ggplot(acs_fit, aes(x=acs_fit$ElectricBill, y=acs_fit$FamilyIncome)) +geom_point() + geom_smooth()
```

# GROUP BY ANALYSIS

```{r}
# This shows a good spread or range of each family type group. This will lend itself well to being included in my analysis
ddply(acs_fit,.(FamilyType),summarise,family_type_count=length(FamilyIncome))
```
```{r}
# Interesting look at mean income of family type grouped with home ownership type
ddply(acs_fit,.(FamilyType,OwnRent), summarise, mean_income=mean(FamilyIncome))
```
```{r}
ddply(acs_fit,.(FamilyType,FoodStamp), summarise, mean_income=mean(FamilyIncome))
```
```{r}
# simple look at mean income by foodstamp. Obvious results, but at the same time surprising to find that mean income 
# for those on food stamps in near $50k
ddply(acs_fit,.(FoodStamp), summarise, mean_income=mean(FamilyIncome))
```
```{r}
ddply(acs_fit,.(FoodStamp,NumBedrooms), summarise, mean_income=mean(FamilyIncome), num_bedrooms=mean(NumBedrooms))
```
```{r}
#ddply(acs,.(NumBedrooms,NumChildren,NumPeople,NumRooms,NumUnits,NumVehicles,NumWorkers), summarise, mean_income=mean(FamilyIncome))
```
```{r}
ddply(acs_fit,.(OwnRent), summarise, mean_income=mean(FamilyIncome))
```

# COUNT (FAMILY INCOME) BY VARIOUS IMPORTANT INDICATORS/VARIABLES

```{r}
# Family Type
tapply(acs_fit$FamilyIncome,acs_fit$FamilyType,length)
tapply(acs_fit$FamilyIncome,acs_fit$FamilyType,mean)
```

```{r}
# Own/Rent
tapply(acs_fit$FamilyIncome,acs_fit$OwnRent,length)
tapply(acs_fit$FamilyIncome,acs_fit$OwnRent,mean)
```

```{r}
# Insurance
tapply(acs_fit$FamilyIncome,acs_fit$FoodStamp,length)
tapply(acs_fit$FamilyIncome,acs_fit$FoodStamp,mean)
```

# Task 3 - Building predictive classifier models using the entire training dataset

Let's start by building a *null* model in which you simply predict that everyone's
income is < 250000 (since the majority of incomes are less than 250000).

```{r}
acs$null_model <- 0
```


Create a confusion matrix table and compute the overall accuracy of this model
as well as its sensitivity and specificity.

```{r}
library(caret)
table(acs$HighIncome, acs$null_model)
prop.table(table(acs$HighIncome, acs$null_model))
```
```{r}
confusionMatrix(as.factor(acs$null_model), as.factor(acs$HighIncome), positive = "1")
```


We would like to build a more accurate model than this.
Your job is to build classifiers to predict the binary HighIncome we created. 
You will be using three different classification
techniques:
* decision trees (use `rpart` package - see Kaggle Titanic example from StatModels2 or session on trees)
* logistic regression (see logistic regression examples we did in StatModels2)
* k-nearest neighbor or some other technique (see kNN example we did in StatModels2)
For each technique, you should:
* build a few models with the training data
* create confusion matrices (using `caret` package) to pick the best fit model for each technique
* use your three best fit models (one from each technique) to predict using the test dataset and evaluate which of the models performs the best
* write a few paragraphs discussing what you did and what you found. In particular, how difficult is it to predict HighIncome? Did one of the techniques outperform the other two?

# LOGISTIC REGRESSION
```{r}
#acs$logmod1 <- 0
#table(acs_fit$HighIncome, acs_fit$logmod1)
#prop.table(table(acs_fit$HighIncome, acs_fit$logmod1))

# logistic regression model 1
logmod1 <- glm(HighIncome ~ FamilyType + NumVehicles + OwnRent + Insurance + YearBuilt, data=acs_fit, family=binomial(link="logit"))
# show regression output/summary stats
summary(logmod1)
# fitting values based on model
# check fitted values as probility
logmod1$fitted.values[1:10]
yhat_logmod1 <- (logmod1$fitted.values > 0.05) * 1
# Show fitted values to make sure code is working correctly
logmod1$y[1:200]
#acs_test$lm1_yhat <- ifelse(logmod1$fitted.values >=.5,1,0) - I DON'T RECALL WHAT THIS IS FOR
# confusion matrux for this model
#run a table first
summary(as.factor(yhat_logmod1))
summary(logmod1$fitted.values)
#table(as.factor(acs_fit$yhat_logmod1), as.factor(acs_fit$HighIncome), positive = "1")
confusionMatrix(as.factor(yhat_logmod1), as.factor(acs_fit$HighIncome), positive = "1")
```

```{r}
# logistic regression model 2
logmod2 <- glm(HighIncome ~ FamilyType + FoodStamp + OwnRent, data=acs_fit, family=binomial(link="logit"))
summary(logmod2)
logmod2$fitted.values[100:150]
yhat_logmod2 <- (logmod2$fitted.values > 0.5) * 1
logmod2$y[1:50]
confusionMatrix(as.factor(yhat_logmod2), as.factor(acs_fit$HighIncome), positive = "1")
```

```{r}
# logistic regression model 3
logmod3 <- glm(HighIncome ~ InsuranceHigh + NumWorkers2 + HouseCostsHigh + FoodStamp + OwnRent, data=acs_fit, family=binomial(link="logit"))
summary(logmod3)
logmod3$fit[18100:18196]
yhat_logmod3 <- (logmod3$fit > 0.5) * 1
logmod3$y[18000:18196]
confusionMatrix(as.factor(yhat_logmod3), as.factor(acs_fit$HighIncome), positive = "1")
```

```{r}
logmod4 <- glm(HighIncome ~ InsuranceHigh + NumWorkers2 + HouseCostsHigh, data=acs_fit, family=binomial(link="logit"))
summary(logmod4)
logmod4$fit[18100:18196]
yhat_logmod4 <- (logmod4$fit > 0.5) * 1
logmod4$y[18000:18196]
confusionMatrix(as.factor(yhat_logmod4), as.factor(acs_fit$HighIncome), positive = "1")
```
```{r}

logmod5 <- glm(HighIncome ~ HouseCostsHigh, data=acs_fit, family=binomial(link="logit"))
summary(logmod5)
logmod5$fit[18100:18196]
yhat_logmod5 <- (logmod4$fit > 0.5) * 1
logmod5$y[18000:18196]
confusionMatrix(as.factor(yhat_logmod5), as.factor(acs_fit$HighIncome), positive = "1")
```


```{r}
# Created a linear regression for fun to estimate family income with test data set
linear_mod1 <- lm(FamilyIncome ~ FamilyType + FoodStamp + OwnRent + HouseCosts + Insurance + ElectricBill + NumRooms, data=acs_fit)
summary(linear_mod1)
```

```{r}
# Created a linear regression for fun to estimate family income with test data set
linear_mod2 <- lm(FamilyIncome ~ HI_pred1 + HouseCosts + Insurance + high_electric + NumRooms, data=acs_fit)
summary(linear_mod2)
```


# DECISION TREES

```{r}
tree1 <- rpart(HighIncome ~ FamilyType + HouseCosts + NumWorkers2 + OwnRent + Insurance + NumWorkers2 + YearBuilt + NumBedrooms, data=acs_fit, method="class")
rpart.plot(tree1)
tree1
# Predictions
tree_pred1 <- predict(tree1, acs_test, type = "class")
summary(tree_pred1)
```
```{r}
# Don't run this tree!
#tree2 <- rpart(HighIncome ~ FoodStamp + Insurance + FamilyType, data=acs_fit, method="class", control=rpart.control(minsplit=2, cp=0))
#rpart.plot(tree2)
#tree2
#tree_pred2 <- predict(tree2, acs_test, type = "class")
#summary(tree_pred2)
```
```{r}
tree3 <- rpart(HighIncome ~ Insurance + ElectricBill + HouseCosts, data=acs_fit, method="class", control=rpart.control(minsplit=2, cp=.005))
rpart.plot(tree3)
tree3
tree_pred3 <- predict(tree3, acs_test, type = "class")
summary(tree_pred3)
```

```{r}
tree4 <- rpart(HighIncome ~ Insurance + ElectricBill + HouseCosts + NumBedrooms + NumChildren + NumPeople + NumRooms + NumVehicles + NumWorkers + FoodStamp + OwnRent + ElectricBill + HouseCosts, data=acs_fit, method="class", control=rpart.control(minsplit=2, cp=0))
rpart.plot(tree4)
tree4
tree_pred4 <- predict(tree4, acs_test, type = "class")
summary(tree_pred4)
```

# k-nearest neighbor

```{r}

```


## HACKER EXTRAS

There's tons of opportunities for extra credit here. A couple of ideas are:

* try other classification techniques (see the ISLR textbook ([http://www-bcf.usc.edu/~gareth/ISL/](http://www-bcf.usc.edu/~gareth/ISL/))) such as neural nets, random forests or discriminant analysis
* In Task 3, use k-fold cross-validation for the logistic regression models

You'll be turning in your Rmd file with all of your work.